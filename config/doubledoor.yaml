{
    # Model configuration
    "MODEL_CONFIG": {
        "custom_model": "actor_critic",
        "custom_model_config": {
            "input_conv_channels": 3,
            "critic_share_layers": False,
            "conv_filters": [
                [16, [2, 2], 1],
                ["pool", [2, 2], 2],
                [32, [2, 2], 1],
                [64, [2, 2], 1],
            ],
            "actor_layer_sizes": [[960, 5]],
            "critic_layer_sizes": [[960, 1]],
            "action_masking": True,
        },
    },


    # Environment configuration
    "ENV_CONFIG": {
        "config": [["wwow", "owwd"], ["wwow", "odww"]],
        # "config": [["wwow", "owwo"], ["wwow", "ooww"]],
        "start_rooms": [[0, 0], [0, 1]],
        "goal_rooms": [[1, 0], [1, 1]],
        # "start_rooms": [[0, 0]],
        # "goal_rooms": [[1, 1]],
        "room_size": 3,
        "max_steps": 400,
        "exploration_bonus": False,
    },


    # Base configuration including algorithm parameters
    "BASE_CONFIG": {
        "advice_mode": "llm", # in the set of {"", "llm", "baseline"}
        "smooth_mode": "win",
        "score_mean": 152.674674,
        "score_std": 74.6598866,
        "max_steps": 400,
        "reward_clipping_func": "",
        "max_reward_scale": 10,
        "teacher_model_path": "$SRC/LLM/RM/models/reward_model_llama3.pth",
        "random_network_path": "$SRC/LLM/RM/models/random_model.pth",
        "teacher_model_config": {
            "conv_filters": [
                [16, [2, 2], 1, [2,1]],
                ["pool", [2, 2], 2, [0,0]],
                [32, [2, 2], 1, [0,0]],
                [64, [2, 2], 1, [0,0]]
            ],
            "conv_activation": True,
            "fc_layer_sizes": [[256, 512], [512,256], [256, 128], [128, 64], [64, 16], [16, 1]],
            "clip_at_last": "",
            "clip_scale": 1,
        },
        "env": "doubledoor",
        "alg": "ppo",

        "num_gpus": 0.0,
        "num_cpus_per_worker": 1,
        "num_cpus_for_driver": 1,
        "framework": "torch", # Only torch supported

        "lr": 0.0001, # 0.0005,
        # "lr_schedule":[
        #     [0, 0.005],
        #     [50000, 0.005],
        #     [60000, 0.000001],
        # ],
        "lambda": 0.8,
        "kl_coeff": 0.5,
        "clip_rewards": False,
        "clip_param": 0.2,
        "vf_clip_param": 10.0,
        "vf_loss_coeff": 0.5,
        "entropy_coeff": 0.01,
        "train_batch_size": 2048,
        # "rollout_fragment_length": 100,
        "sgd_minibatch_size": 128,
        "num_sgd_iter": 4,

        # IMPORTANT: This must remain as 0 in order for the teacher's target model to propagate to rollout workers (for now)
        # TODO: Fix by syncing the teacher target policy as a global var, so that workers get updated copies (look at set_global_vars()).
        # "num_workers": 2,
        "num_workers": 4,

        "num_envs_per_worker": 10,
        "batch_mode": "truncate_episodes",
        "observation_filter": "NoFilter",

        "use_gae": True,
    },


    # Hyper parameter optimization parameters
    # "HPO_CONFIG": {
    #     # "introspection_decay_rate": "tune.grid_search([0.9999, 0.999999])",
    #     # "introspection_threshold": "tune.grid_search([0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5])",
    #     "introspection_threshold": "tune.grid_search([0.1, 0.5, 0.9, 1.3])",
    # }
}